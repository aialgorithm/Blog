{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56ad7813",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "项目功能：输入关键字，生成藏头诗。\n",
    "项目配置：\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# 禁用词，包含如下字符的唐诗将被忽略\n",
    "DISALLOWED_WORDS = ['（', '）', '(', ')', '__', '《', '》', '【', '】', '[', ']']\n",
    "# 句子最大长度\n",
    "MAX_LEN = 64\n",
    "# 最小词频\n",
    "MIN_WORD_FREQUENCY = 8\n",
    "# 训练的batch size\n",
    "BATCH_SIZE = 16\n",
    "# 数据集路径\n",
    "DATASET_PATH = './poetry.txt'\n",
    "# 每个epoch训练完成后，随机生成SHOW_NUM首古诗作为展示\n",
    "SHOW_NUM = 5\n",
    "# 共训练多少个epoch\n",
    "TRAIN_EPOCHS = 2\n",
    "# 最佳权重保存路径\n",
    "BEST_MODEL_PATH = './best_model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0eb3ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "构建数据集\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "    \"\"\"\n",
    "    分词器\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, token_dict):\n",
    "        # 词->编号的映射\n",
    "        self.token_dict = token_dict\n",
    "        # 编号->词的映射\n",
    "        self.token_dict_rev = {value: key for key, value in self.token_dict.items()}\n",
    "        # 词汇表大小\n",
    "        self.vocab_size = len(self.token_dict)\n",
    "\n",
    "    def id_to_token(self, token_id):\n",
    "        \"\"\"\n",
    "        给定一个编号，查找词汇表中对应的词\n",
    "        :param token_id: 带查找词的编号\n",
    "        :return: 编号对应的词\n",
    "        \"\"\"\n",
    "        return self.token_dict_rev[token_id]\n",
    "\n",
    "    def token_to_id(self, token):\n",
    "        \"\"\"\n",
    "        给定一个词，查找它在词汇表中的编号\n",
    "        未找到则返回低频词[UNK]的编号\n",
    "        :param token: 带查找编号的词\n",
    "        :return: 词的编号\n",
    "        \"\"\"\n",
    "        return self.token_dict.get(token, self.token_dict['[UNK]'])\n",
    "\n",
    "    def encode(self, tokens):\n",
    "        \"\"\"\n",
    "        给定一个字符串s，在头尾分别加上标记开始和结束的特殊字符，并将它转成对应的编号序列\n",
    "        :param tokens: 待编码字符串\n",
    "        :return: 编号序列\n",
    "        \"\"\"\n",
    "        # 加上开始标记\n",
    "        token_ids = [self.token_to_id('[CLS]'), ]\n",
    "        # 加入字符串编号序列\n",
    "        for token in tokens:\n",
    "            token_ids.append(self.token_to_id(token))\n",
    "        # 加上结束标记\n",
    "        token_ids.append(self.token_to_id('[SEP]'))\n",
    "        return token_ids\n",
    "\n",
    "    def decode(self, token_ids):\n",
    "        \"\"\"\n",
    "        给定一个编号序列，将它解码成字符串\n",
    "        :param token_ids: 待解码的编号序列\n",
    "        :return: 解码出的字符串\n",
    "        \"\"\"\n",
    "        # 起止标记字符特殊处理\n",
    "        spec_tokens = {'[CLS]', '[SEP]'}\n",
    "        # 保存解码出的字符的list\n",
    "        tokens = []\n",
    "        for token_id in token_ids:\n",
    "            token = self.id_to_token(token_id)\n",
    "            if token in spec_tokens:\n",
    "                continue\n",
    "            tokens.append(token)\n",
    "        # 拼接字符串\n",
    "        return ''.join(tokens)\n",
    "\n",
    "\n",
    "# 禁用词\n",
    "disallowed_words = DISALLOWED_WORDS\n",
    "# 句子最大长度\n",
    "max_len = MAX_LEN\n",
    "# 最小词频\n",
    "min_word_frequency = MIN_WORD_FREQUENCY\n",
    "# mini batch 大小\n",
    "batch_size = BATCH_SIZE\n",
    "\n",
    "# 加载数据集\n",
    "with open(DATASET_PATH, 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    # 将冒号统一成相同格式\n",
    "    lines = [line.replace('：', ':') for line in lines]\n",
    "# 数据集列表\n",
    "poetry = []\n",
    "# 逐行处理读取到的数据\n",
    "for line in lines:\n",
    "    # 有且只能有一个冒号用来分割标题\n",
    "    if line.count(':') != 1:\n",
    "        continue\n",
    "    # 后半部分不能包含禁止词\n",
    "    __, last_part = line.split(':')\n",
    "    ignore_flag = False\n",
    "    for dis_word in disallowed_words:\n",
    "        if dis_word in last_part:\n",
    "            ignore_flag = True\n",
    "            break\n",
    "    if ignore_flag:\n",
    "        continue\n",
    "    # 长度不能超过最大长度\n",
    "    if len(last_part) > max_len - 2:\n",
    "        continue\n",
    "    poetry.append(last_part.replace('\\n', ''))\n",
    "\n",
    "# 统计词频\n",
    "counter = Counter()\n",
    "for line in poetry:\n",
    "    counter.update(line)\n",
    "# 过滤掉低频词\n",
    "_tokens = [(token, count) for token, count in counter.items() if count >= min_word_frequency]\n",
    "# 按词频排序\n",
    "_tokens = sorted(_tokens, key=lambda x: -x[1])\n",
    "# 去掉词频，只保留词列表\n",
    "_tokens = [token for token, count in _tokens]\n",
    "\n",
    "# 将特殊词和数据集中的词拼接起来\n",
    "_tokens = ['[PAD]', '[UNK]', '[CLS]', '[SEP]'] + _tokens\n",
    "# 创建词典 token->id映射关系\n",
    "token_id_dict = dict(zip(_tokens, range(len(_tokens))))\n",
    "# 使用新词典重新建立分词器\n",
    "tokenizer = Tokenizer(token_id_dict)\n",
    "# 混洗数据\n",
    "np.random.shuffle(poetry)\n",
    "\n",
    "class PoetryDataGenerator:\n",
    "    \"\"\"\n",
    "    古诗训练数据集生成\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, random=False):\n",
    "        # 数据集\n",
    "        self.data = data\n",
    "        # batch size\n",
    "        self.batch_size = batch_size\n",
    "        # 每个epoch迭代的步数\n",
    "        self.steps = int(math.floor(len(self.data) / self.batch_size))\n",
    "        # 每个epoch开始时是否随机混洗\n",
    "        self.random = random\n",
    "\n",
    "    def sequence_padding(self, data, length=None, padding=None):\n",
    "        \"\"\"\n",
    "        将给定数据填充到相同长度\n",
    "        :param data: 待填充数据\n",
    "        :param length: 填充后的长度，不传递此参数则使用data中的最大长度\n",
    "        :param padding: 用于填充的数据，不传递此参数则使用[PAD]的对应编号\n",
    "        :return: 填充后的数据\n",
    "        \"\"\"\n",
    "        # 计算填充长度\n",
    "        if length is None:\n",
    "            length = max(map(len, data))\n",
    "        # 计算填充数据\n",
    "        if padding is None:\n",
    "            padding = tokenizer.token_to_id('[PAD]')\n",
    "        # 开始填充\n",
    "        outputs = []\n",
    "        for line in data:\n",
    "            padding_length = length - len(line)\n",
    "            # 不足就进行填充\n",
    "            if padding_length > 0:\n",
    "                outputs.append(np.concatenate([line, [padding] * padding_length]))\n",
    "            # 超过就进行截断\n",
    "            else:\n",
    "                outputs.append(line[:length])\n",
    "        return np.array(outputs)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.steps\n",
    "\n",
    "    def __iter__(self):\n",
    "        total = len(self.data)\n",
    "        # 是否随机混洗\n",
    "        if self.random:\n",
    "            np.random.shuffle(self.data)\n",
    "        # 迭代一个epoch，每次yield一个batch\n",
    "        for start in range(0, total, self.batch_size):\n",
    "            end = min(start + self.batch_size, total)\n",
    "            batch_data = []\n",
    "            # 逐一对古诗进行编码\n",
    "            for single_data in self.data[start:end]:\n",
    "                batch_data.append(tokenizer.encode(single_data))\n",
    "            # 填充为相同长度\n",
    "            batch_data = self.sequence_padding(batch_data)\n",
    "            # yield x,y\n",
    "            yield batch_data[:, :-1], tf.one_hot(batch_data[:, 1:], tokenizer.vocab_size)\n",
    "            del batch_data\n",
    "\n",
    "    def for_fit(self):\n",
    "        \"\"\"\n",
    "        创建一个生成器，用于训练\n",
    "        \"\"\"\n",
    "        # 死循环，当数据训练一个epoch之后，重新迭代数据\n",
    "        while True:\n",
    "            # 委托生成器\n",
    "            yield from self.__iter__()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4691af27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb0cb509",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_acrostic(tokenizer, model, head):\n",
    "    \"\"\"\n",
    "    随机生成一首藏头诗\n",
    "    :param tokenizer: 分词器\n",
    "    :param model: 用于生成古诗的模型\n",
    "    :param head: 藏头诗的头\n",
    "    :return: 一个字符串，表示一首古诗\n",
    "    \"\"\"\n",
    "    # 使用空串初始化token_ids，加入[CLS]\n",
    "    token_ids = tokenizer.encode('')\n",
    "    token_ids = token_ids[:-1]\n",
    "    # 标点符号，这里简单的只把逗号和句号作为标点\n",
    "    punctuations = ['，', '。']\n",
    "    punctuation_ids = {tokenizer.token_to_id(token) for token in punctuations}\n",
    "    # 缓存生成的诗的list\n",
    "    poetry = []\n",
    "    # 对于藏头诗中的每一个字，都生成一个短句\n",
    "    for ch in head:\n",
    "        # 先记录下这个字\n",
    "        poetry.append(ch)\n",
    "        # 将藏头诗的字符转成token id\n",
    "        token_id = tokenizer.token_to_id(ch)\n",
    "        # 加入到列表中去\n",
    "        token_ids.append(token_id)\n",
    "        # 开始生成一个短句\n",
    "        while True:\n",
    "            # 进行预测，只保留第一个样例（我们输入的样例数只有1）的、最后一个token的预测的、不包含[PAD][UNK][CLS]的概率分布\n",
    "            output = model(np.array([token_ids, ], dtype=np.int32))\n",
    "            _probas = output.numpy()[0, -1, 3:]\n",
    "            del output\n",
    "            # 按照出现概率，对所有token倒序排列\n",
    "            p_args = _probas.argsort()[::-1][:100]\n",
    "            # 排列后的概率顺序\n",
    "            p = _probas[p_args]\n",
    "            # 先对概率归一\n",
    "            p = p / sum(p)\n",
    "            # 再按照预测出的概率，随机选择一个词作为预测结果\n",
    "            target_index = np.random.choice(len(p), p=p)\n",
    "            target = p_args[target_index] + 3\n",
    "            # 保存\n",
    "            token_ids.append(target)\n",
    "            # 只有不是特殊字符时，才保存到poetry里面去\n",
    "            if target > 3:\n",
    "                poetry.append(tokenizer.id_to_token(target))\n",
    "            if target in punctuation_ids:\n",
    "                break\n",
    "    return ''.join(poetry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cb04e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 128)         439552    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, None, 128)         131584    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, None, 128)         131584    \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, None, 3434)        442986    \n",
      "=================================================================\n",
      "Total params: 1,145,706\n",
      "Trainable params: 1,145,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "构建LSTM模型\n",
    "\n",
    "\"\"\"\n",
    "model = tf.keras.Sequential([\n",
    "    # 不定长度的输入\n",
    "    tf.keras.layers.Input((None,)),\n",
    "    # 词嵌入层\n",
    "    tf.keras.layers.Embedding(input_dim=tokenizer.vocab_size, output_dim=128),\n",
    "    # 第一个LSTM层，返回序列作为下一层的输入\n",
    "    tf.keras.layers.LSTM(128, dropout=0.5, return_sequences=True),\n",
    "    # 第二个LSTM层，返回序列作为下一层的输入\n",
    "    tf.keras.layers.LSTM(128, dropout=0.5, return_sequences=True),\n",
    "    # 对每一个时间点的输出都做softmax，预测下一个词的概率\n",
    "    tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(tokenizer.vocab_size, activation='softmax')),\n",
    "])\n",
    "\n",
    "# 查看模型结构\n",
    "model.summary()\n",
    "# 配置优化器和损失函数\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.categorical_crossentropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed707add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-3d82ab0655a5>:30: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/2\n",
      "1534/1534 [==============================] - ETA: 0s - loss: 4.8169cun'h\n",
      "春叶风烟有，花客长深里。秋家人落香，月里千不烟。\n",
      "春知夜山下，花远别南天。秋事云入衣，月客云入，\n",
      "春玉心来色，花云马人客。秋有复归人，月上未夜里。\n",
      "春多何月客，花千江尽思。秋里夜年多，月山山日，\n",
      "春日海秋东，花日有酒中。秋客心花客，月来深烟然。\n",
      "1534/1534 [==============================] - 152s 99ms/step - loss: 4.8169\n",
      "Epoch 2/2\n",
      "1534/1534 [==============================] - ETA: 0s - loss: 4.3516cun'h\n",
      "春山千年落，花舟竹林回。秋寺千不处，月事入清分。\n",
      "春南清阳望，花心出自闲。秋云江望满，月路满江归。\n",
      "春日流江里，花似楚乡声。秋水落里冷，月落秋天同。\n",
      "春里何北在，花人去家飞。秋闻何高发，月灯老中流。\n",
      "春来无上尽，花月寄上开。秋花夜日下，月边有门深。\n",
      "1534/1534 [==============================] - 153s 100ms/step - loss: 4.3516\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x212f48c3ca0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "模型训练\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class Evaluate(tf.keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    训练过程评估，在每个epoch训练完成后，保留最优权重，并随机生成SHOW_NUM首古诗展示\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 给loss赋一个较大的初始值\n",
    "        self.lowest = 1e10\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # 在每个epoch训练完成后调用\n",
    "        # 如果当前loss更低，就保存当前模型参数\n",
    "        if logs['loss'] <= self.lowest:\n",
    "            self.lowest = logs['loss']\n",
    "            model.save(BEST_MODEL_PATH)\n",
    "        # 随机生成几首古体诗测试，查看训练效果\n",
    "        print(\"cun'h\")\n",
    "        for i in range(SHOW_NUM):\n",
    "            print(generate_acrostic(tokenizer, model, head=\"春花秋月\"))\n",
    "\n",
    "# 创建数据集\n",
    "data_generator = PoetryDataGenerator(poetry, random=True)\n",
    "# 开始训练\n",
    "model.fit_generator(data_generator.for_fit(), steps_per_epoch=data_generator.steps, epochs=TRAIN_EPOCHS,\n",
    "                    callbacks=[Evaluate()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78aa8bb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c41a98b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入关键字:\n",
      "算法进阶\n",
      "算十仙年山，法是寄一林。进门南来满，阶山忆寒声。 \n",
      "\n",
      "算尽来路宿，法月几如枝。进寒无年事，阶声复里开。 \n",
      "\n",
      "算知朝南上，法门与声愁。进觉归云火，阶照树人头。 \n",
      "\n",
      "算人春下远，法如更如中。进是归中别，阶落鸟归残。 \n",
      "\n",
      "算夜白深石，法为玉神天。进随秋来处，阶中暮处迟。 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "输入关键字，生成藏头诗\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# 加载训练好的模型\n",
    "model = tf.keras.models.load_model(BEST_MODEL_PATH)\n",
    "\n",
    "keywords = input('输入关键字:\\n')\n",
    "\n",
    "\n",
    "# 生成藏头诗\n",
    "for i in range(SHOW_NUM):\n",
    "    print(generate_acrostic(tokenizer, model, head=keywords),'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccf30a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6946b36c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e33c3f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de3524f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59e83e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
